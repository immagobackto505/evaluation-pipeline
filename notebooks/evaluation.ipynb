{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "627e39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate, sacrebleu\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf18ee",
   "metadata": {},
   "source": [
    "### Get respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb92523",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instruction = pd.read_csv('Thai_Chinese_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key='‡πçYOUR_KEY')\n",
    "\n",
    "start_index = 0\n",
    "batch_num = 1\n",
    "\n",
    "for index, row in tqdm(Instruction.iterrows()):\n",
    "    \n",
    "    '''\n",
    "    This loop generates responses for each instruction in the Instruction DataFrame using the Gemini-2.5-flash model.\n",
    "    It saves the responses in batches of 10 to CSV files in the 'respond_batch' directory.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # print(f\"Processing row {index} \\n\")\n",
    "\n",
    "    prompt = f\"\"\"You are an intelligent language model. \n",
    "    Follow the instruction carefully and respond concisely.\n",
    "\n",
    "    Instruction: {row['instruction']}\n",
    "    Input: \"{row['input']}\"\n",
    "    Output:\"\"\"\n",
    "\n",
    "    # print(prompt, '\\n')\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    )\n",
    "    \n",
    "    Instruction.loc[index, 'respond'] = response.text\n",
    "\n",
    "    if (index+1) % 10 == 0 or (index+1) == len(Instruction):\n",
    "        print(f'BATCH {batch_num} SAVE : from {start_index} to {index}')\n",
    "        batch_df = Instruction.iloc[start_index:index+1]\n",
    "        os.makedirs(\"respond_batch\", exist_ok=True)\n",
    "        batch_df.to_csv(f\"respond_batch/batch_{batch_num}.csv\", index=True)\n",
    "        batch_num += 1\n",
    "        start_index = index+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f849631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the bacthes into one csv file\n",
    "files = glob.glob(\"respond_batch/batch_*.csv\")\n",
    "respond_df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\n",
    "respond_df.to_csv(\"outputs/responded_dataset.csv\", index=False)\n",
    "respond_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56629199",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a846bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"outputs/responded_dataset.csv\") #llm responses csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bb438",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrF++ per-sample (solid for ZH/TH)\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "df[\"chrf\"] = [\n",
    "    chrf.compute(predictions=[h], references=[[r]])[\"score\"]\n",
    "    for h, r in tqdm(list(zip(df[\"respond\"], df[\"ref\"])),\n",
    "                     total=len(df), desc=\"chrF\")\n",
    "]\n",
    "\n",
    "# sentence BLEU\n",
    "df[\"bleu_sent\"] = [\n",
    "    sacrebleu.sentence_bleu(h, [r]).score\n",
    "    for h, r in tqdm(list(zip(df[\"respond\"], df[\"ref\"])),\n",
    "                     total=len(df), desc=\"BLEU (sent)\")\n",
    "]\n",
    "\n",
    "# corpus BLEU\n",
    "corpus_bleu = sacrebleu.corpus_bleu(df[\"respond\"].tolist(), [df[\"ref\"].tolist()]).score\n",
    "\n",
    "# Optional semantic similarity\n",
    "try:\n",
    "    bs = evaluate.load(\"bertscore\")\n",
    "    df[\"bertscore_f1\"] = bs.compute(\n",
    "        predictions=df[\"respond\"].tolist(),\n",
    "        references=df[\"ref\"].tolist(),\n",
    "        lang=\"th\"  # language of the hypothesis strings\n",
    "    )[\"f1\"]\n",
    "except Exception as e:\n",
    "    print(\"Skipping BERTScore (install torch + bert-score to enable). Reason:\", e)\n",
    "\n",
    "print(\"\\n=== Qwen corpus summary ===\")\n",
    "print(f\"chrF++ avg     : {df['chrf'].mean():.3f}\")\n",
    "print(f\"BLEU (avg)     : {df['bleu_sent'].mean():.3f}\")\n",
    "print(f\"BLEU (corpus)  : {corpus_bleu:.3f}\")\n",
    "if \"bertscore_f1\" in df:\n",
    "    print(f\"BERTScore F1   : {df['bertscore_f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55949c39",
   "metadata": {},
   "source": [
    "### Save evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputs/evaluations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e5761",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34865e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('outputs/evaluations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50666553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()[['type','chrf', 'bleu_sent', 'bertscore_f1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba45faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add domain column\n",
    "df['domain'] = df['type'].str.replace('.json', '').str.replace('-', '_').str.split('_').str[0]\n",
    "df['domain'] = df['domain'].str.title().replace({\n",
    "    'word': 'Word_Alignment',\n",
    "    'Partial': 'Partial_Translation'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('domain')[['chrf', 'bleu_sent', 'bertscore_f1']].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df455765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputs/evaluations_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
